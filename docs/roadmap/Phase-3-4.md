TapLive XR Search Engine
A global XR telepresence and sensing network powered by LBS, OpenXR, and human–device collaboration.
---
Motivation — Why We Started
TapLive began with a simple but powerful question:
“What if anyone, anywhere, could instantly experience a remote place through the eyes of a human or a device — in XR?”
During daily life and travel, we often find ourselves wanting to see through someone else’s perspective:
What does this street look like right now?
Can someone check a location for me?
What does a real-time IoT sensor or drone see?
How can we collaborate remotely in complex environments?
The existing internet provides information and videos — but not real-time presence.
And existing XR systems provide immersion — but not global, on-demand access to real humans, devices, and sensors.
TapLive XR Search was born from this gap.
---
Vision — Turning the Physical World into a Searchable XR Layer
Our idea evolved into a core concept:
A global XR search engine that connects humans, cameras, drones, and IoT sensors through LBS and OpenXR, enabling real-time remote vision and collaboration.
Instead of searching text or images, you “search” live presence:
People streaming from XR headsets
Phones and cameras available for on-demand telepresence
IoT sensors providing real-time environmental data
Drones performing 360° remote inspections
MR-assisted fieldwork tasks
Night-vision, thermal imaging, remote surveying, etc.
This blends LBS + XR + IoT + Telepresence into a single, unified experience.
---
How We Built It
To demonstrate this vision within the hackathon timeframe, we focused on a minimal but powerful XR prototype:
1. LBS-based discovery
Users can locate available XR points — whether they are people, devices, or sensors — directly on a map.
2. OpenXR integration
We connected XR devices (e.g., VR/AR/MR headsets) using OpenXR to ensure interoperability and future scalability.
3. Real-time telepresence
We implemented low-latency streaming from phones, cameras, and IoT endpoints into XR, allowing users to “enter” a remote perspective.
4. IoT + sensing bridge
A lightweight protocol connects sensors (thermal, night vision, environmental, drone feeds) into the same XR view.
Although early-stage, this prototype forms the foundation of Phase 3–4 of TapLive — our long-term XR direction.
---
Challenges We Encountered
Building TapLive XR Search was not easy. Some of our major challenges:
1. Multi-device interoperability
XR, mobile devices, drones, and sensors all use different standards.
OpenXR helped, but real-world integration still required custom bridging.
2. Latency and rendering performance
Achieving smooth telepresence in XR demanded aggressive optimization, especially across networks and devices.
3. Designing human–IoT collaboration
How should XR users interact with sensors?
How do we visualize mixed data sources naturally in XR?
We iterated multiple times to find a balanced UI/UX.
4. Hard limits of time
We were still developing our Phase 1–2 live order system.
Jumping directly to XR (Phase 3–4) meant compressing complex concepts into a minimal prototype — but it pushed us creatively.
---
What We Learned
Through this process, we gained insights:
XR is not an isolated medium — it becomes exponentially more powerful when connected to the real world.
LBS is a natural backbone for global telepresence.
IoT + XR → new collaboration models for surveying, education, entertainment, inspection, rescue, and more.
Hackathons reward innovation over completeness, allowing ambitious vision to be prototyped early.
And most importantly:
People want to see the world through other eyes — and XR makes this natural.
---
What’s Next
TapLive XR Search is just the beginning.
Our next steps:
1. Expand OpenXR interoperability
2. Add multi-user XR collaboration
3. Integrate drone and sensor networks more deeply
4. Build an AI-assisted “XR routing” engine
5. Grow the global telepresence map into a true spatial browser for the real world
Our long-term goal is to construct a planet-scale XR presence network that augments human perception and enables new forms of collaboration across distance, geography, and reality layers.